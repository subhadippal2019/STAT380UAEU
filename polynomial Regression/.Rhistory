model_ss <- lm (MEDV ~ bs(LSTAT, knots = knots, degree=2),  data =train)
summary(model_ss)
ggplot(train, aes(LSTAT, MEDV) ) + geom_point() +
stat_smooth(method = lm, formula = y ~ splines::bs(x, knots = knots, degree=2))
#install.packages("splines")
library(splines)
# Build the model
knots <- quantile(train$LSTAT, p = c(0.25, .5,  0.75))
model_ss <- lm (MEDV ~ bs(LSTAT, knots = knots),  data =train)
summary(model_ss)
ggplot(train, aes(LSTAT, MEDV) ) + geom_point() +
stat_smooth(method = lm, formula = y ~ splines::bs(x, knots = knots))
#install.packages("splines")
library(splines)
# Build the model
knots <- quantile(train$LSTAT, p = c(0.25, .5,  0.9))
model_ss <- lm (MEDV ~ bs(LSTAT, knots = knots, degree=2),  data =train)
summary(model_ss)
ggplot(train, aes(LSTAT, MEDV) ) + geom_point() +
stat_smooth(method = lm, formula = y ~ splines::bs(x, knots = knots, degree=2))
#install.packages("splines")
library(splines)
# Build the model
knots <- quantile(train$LSTAT, p = c(0.25, .5,  0.75))
model_ss <- lm (MEDV ~ bs(LSTAT, knots = knots),  data =train)
summary(model_ss)
ggplot(train, aes(LSTAT, MEDV) ) + geom_point() +
stat_smooth(method = lm, formula = y ~ splines::bs(x, knots = knots))
#predictions <- modelss %>% predict(test)
predictions <-  predict(model_ss3, test)
predict(model_ss3, test)
#install.packages("splines")
library(splines)
# Build the model
knots <- quantile(train$LSTAT, p = c(0.25, .5,  0.75))
model_ss3 <- lm (MEDV ~ bs(LSTAT, knots = knots),  data =train)
summary(model_ss3)
ggplot(train, aes(LSTAT, MEDV) ) + geom_point() +
stat_smooth(method = lm, formula = y ~ splines::bs(x, knots = knots))
#predictions <- modelss %>% predict(test)
predictions <-  predict(model_ss3, test)
# Model performance
data.frame(
RMSE = RMSE(predictions, test$MEDV),
R2 = R2(predictions, test$MEDV))
#predictions <- modelss %>% predict(test)
predictions <-  predict(model_ss3, test)
plot(test$MEDV , predictions)
# Model performance
data.frame(
RMSE = RMSE(predictions, test$MEDV),
R2 = R2(predictions, test$MEDV))
#predictions <- modelss %>% predict(test)
predictions <-  predict(model_ss3, test)
plot(test$MEDV , predictions)
# Model performance
data.frame(
RMSE = RMSE(predictions, test$MEDV),
R2 = R2(predictions, test$MEDV))
fit<-glm(MEDV~CRIM+RM+PTRATIO,data=Daten)
fit<-glm(MEDV~CRIM+RM+PTRATIO,data=Daten)
# Leave-one-out cross-validation
cv_one_err<-cv.glm(Daten,fit)
install.packages("boot")
library(boot)
library(boot)
fit<-glm(MEDV~CRIM+RM+PTRATIO,data=Daten)
# Leave-one-out cross-validation
cv_one_err<-cv.glm(Daten,fit)
cv_one_err$delta
fit<-glm(MEDV~CRIM+RM+PTRATIO,data=Daten)
# Leave-one-out cross-validation
library(boot)
cv_one_err<-cv.glm(Daten,fit)
cv_one_err$delta
help(cv.glm)
cv.glm(Daten,fit)
cv_one_err$call
cv_one_err$K
cv_one_err$delta
fit<-glm(MEDV~CRIM+RM+PTRATIO,data=Daten)
# Leave-one-out cross-validation
library(boot)
cv_one_err<-cv.glm(Daten,fit)
cv_one_err$delta
cv_5_err<-cv.glm(Daten,fit,K=2)
cv_5_err$delta
fit<-glm(MEDV~CRIM+RM+PTRATIO,data=Daten)
library(boot)
# Leave-one-out cross-validation
cv_one_err<-cv.glm(Daten,fit)
cv_one_err$delta
# 5 fold Cross Validation
cv_5_err<-cv.glm(Daten,fit,K=5)
cv_5_err$delta
fit<-glm(MEDV~CRIM+RM+PTRATIO,data=Daten)
library(boot)
# Leave-one-out cross-validation
cv_one_err<-cv.glm(Daten,fit)
cv_one_err$delta[2]
# 5 fold Cross Validation
cv_5_err<-cv.glm(Daten,fit,K=5)
cv_5_err$delta[2]
fit<-glm(MEDV~CRIM+RM+PTRATIO,data=Daten)
library(boot)
# Leave-one-out cross-validation
cv_one_err<-cv.glm(Daten,fit)
cv_one_err$delta
# 5 fold Cross Validation
cv_5_err<-cv.glm(Daten,fit,K=5)
cv_5_err$delta
fit<-glm(MEDV~CRIM+RM+PTRATIO,data=Daten)
library(boot)
# Leave-one-out cross-validation
cv_one_err<-cv.glm(Daten,fit)
cv_one_err$delta
# 5 fold Cross Validation
cv_5_err<-cv.glm(Daten,fit,K=1)
fit<-glm(MEDV~CRIM+RM+PTRATIO,data=Daten)
library(boot)
# Leave-one-out cross-validation
cv_one_err<-cv.glm(Daten,fit)
cv_one_err$delta
# 5 fold Cross Validation
cv_5_err<-cv.glm(Daten,fit,K=10)
cv_5_err$delta
fit<-glm(MEDV~CRIM+RM+PTRATIO,data=Daten)
library(boot)
# Leave-one-out cross-validation
cv_one_err<-cv.glm(Daten,fit)
cv_one_err$delta
# 5 fold Cross Validation
cv_5_err<-cv.glm(Daten,fit,K=406)
cv_5_err$delta
fit<-glm(MEDV~CRIM+RM+PTRATIO,data=Daten)
library(boot)
# Leave-one-out cross-validation
cv_one_err<-cv.glm(Daten,fit)
cv_one_err$delta
# 5 fold Cross Validation
cv_5_err<-cv.glm(Daten,fit,K=200)
cv_5_err$delta
fit<-glm(MEDV~CRIM+RM+PTRATIO,data=Daten)
library(boot)
# Leave-one-out cross-validation
cv_one_err<-cv.glm(Daten,fit)
cv_one_err$delta
# 5 fold Cross Validation
cv_5_err<-cv.glm(Daten,fit,K=5)
cv_5_err$delta
set.seed(10)
fit<-glm(MEDV~CRIM+RM+PTRATIO,data=Daten)
library(boot)
# Leave-one-out cross-validation
cv_one_err<-cv.glm(Daten,fit)
cv_one_err$delta
# 5 fold Cross Validation
cv_5_err<-cv.glm(Daten,fit,K=5)
cv_5_err$delta
set.seed(10)
fit<-glm(MEDV~CRIM+RM+PTRATIO,data=Daten)
library(boot)
# Leave-one-out cross-validation
cv_one_err<-cv.glm(Daten,fit)
cv_one_err$delta
# 5 fold Cross Validation
cv_5_err<-cv.glm(Daten,fit,K=5)
cv_5_err$delta
set.seed(11)
fit<-glm(MEDV~CRIM+RM+PTRATIO,data=Daten)
library(boot)
# Leave-one-out cross-validation
cv_one_err<-cv.glm(Daten,fit)
cv_one_err$delta
# 5 fold Cross Validation
cv_5_err<-cv.glm(Daten,fit,K=5)
cv_5_err$delta
mean((fit$fitted.values-Daten$MEDV)^2)
cv_error<-NULL
for(i in 1:6){
fit_poly<-glm(MEDV~poly(CRIM,degree=i)+RM+PTRATIO,
data=Daten)
cv_error[i]<-cv.glm(Daten,fit_poly,K=10)$delta[1]
}
cv_error
#[1] 34.708 34.813 35.076 34.423 41.874 51.530
cv_error<-NULL
for(i in 1:6){
fit_poly<-glm(MEDV~poly(CRIM,degree=i)+RM+PTRATIO,
data=Daten)
cv_error[i]<-cv.glm(Daten,fit_poly,K=10)$delta[1]
}
cv_error
#[1] 34.708 34.813 35.076 34.423 41.874 51.530
plot(1:6, cv_error)
cv_error<-NULL
for(i in 1:6){
fit_poly<-glm(MEDV~poly(CRIM,degree=i)+RM+PTRATIO,
data=Daten)
cv_error[i]<-cv.glm(Daten,fit_poly,K=10)$delta[1]
}
cv_error
#[1] 34.708 34.813 35.076 34.423 41.874 51.530
plot(1:6, cv_error, type='l')
cv_error<-NULL
for(i in 1:6){
fit_poly<-glm(MEDV~poly(CRIM,degree=i)+RM+PTRATIO,
data=Daten)
cv_error[i]<-cv.glm(Daten,fit_poly,K=10)$delta[1]
}
cv_error
#[1] 34.708 34.813 35.076 34.423 41.874 51.530
plot(1:6, cv_error)
cv_error<-NULL
for(i in 1:6){
fit_poly<-glm(MEDV~poly(CRIM,degree=i)+RM+PTRATIO,
data=Daten)
cv_error[i]<-cv.glm(Daten,fit_poly,K=5)$delta[1]
}
cv_error
#[1] 34.708 34.813 35.076 34.423 41.874 51.530
plot(1:6, cv_error)
cv_error<-NULL
for(i in 1:6){
fit_poly<-glm(MEDV~poly(CRIM,degree=i)+RM+PTRATIO,
data=Daten)
cv_error[i]<-cv.glm(Daten,fit_poly,K=100)$delta[1]
}
cv_error
#[1] 34.708 34.813 35.076 34.423 41.874 51.530
plot(1:6, cv_error)
cv_error<-NULL
for(i in 1:6){
fit_poly<-glm(MEDV~poly(CRIM,degree=i)+RM+PTRATIO,
data=Daten)
cv_error[i]<-cv.glm(Daten,fit_poly,K=10)$delta[1]
}
cv_error
#[1] 34.708 34.813 35.076 34.423 41.874 51.530
plot(1:6, cv_error)
cv_error<-NULL
for(i in 1:6){
fit_poly<-glm(MEDV~poly(CRIM,degree=i)+RM+PTRATIO,
data=Daten)
cv_error[i]<-cv.glm(Daten,fit_poly,K=10)$delta[1]
}
cv_error
#[1] 34.708 34.813 35.076 34.423 41.874 51.530
plot(1:6, cv_error)
cv_error<-NULL
for(i in 1:6){
fit_poly<-glm(MEDV~poly(CRIM,degree=i)+RM+PTRATIO,
data=Daten)
cv_error[i]<-cv.glm(Daten,fit_poly,K=10)$delta[1]
}
cv_error
#[1] 34.708 34.813 35.076 34.423 41.874 51.530
plot(1:6, cv_error)
par(new=TRUE)
plot(1:6, cv_error, type="l")
cv_error<-NULL
for(i in 1:6){
fit_poly<-glm(MEDV~poly(CRIM,degree=i)+RM+PTRATIO,
data=Daten)
cv_error[i]<-cv.glm(Daten,fit_poly,K=10)$delta[1]
}
cv_error
#[1] 34.708 34.813 35.076 34.423 41.874 51.530
plot(1:6, cv_error)
par(new=TRUE)
plot(1:6, cv_error, type="l", col="blue")
cv_error<-NULL
for(i in 1:6){
fit_poly<-glm(MEDV~poly(CRIM,degree=i)+RM+PTRATIO,
data=Daten)
cv_error[i]<-cv.glm(Daten,fit_poly,K=5)$delta[1]
}
cv_error
#[1] 34.708 34.813 35.076 34.423 41.874 51.530
plot(1:6, cv_error)
par(new=TRUE)
plot(1:6, cv_error, type="l", col="blue")
cv_error<-NULL
for(i in 1:6){
fit_poly<-glm(MEDV~poly(CRIM,degree=i)+RM+PTRATIO,data=Daten)
cv_error[i]<-cv.glm(Daten,fit_poly,K=5)$delta[1]
}
cv_error
#[1] 34.708 34.813 35.076 34.423 41.874 51.530
plot(1:6, cv_error)
par(new=TRUE)
plot(1:6, cv_error, type="l", col="blue")
library(glmnet)
#install.packages("glmnet"
library(glmnet)
install.packages("forecast")
install.packages("forecast")
install.packages("forecast")
library(forecast)
install.packages("forecast")
library(forecast)
library(forecast)
knitr::opts_chunk$set(echo = TRUE)
#Make predictions
library(forecast)
predictions_test =  predict(object =model_polynomial,newdata= test )
#MOdel performance on the Testing Data
data.frame(RMSE = RMSE(predictions_test, test$MEDV), R2 = R2(predictions_test, test$MEDV))
knitr::opts_chunk$set(echo = TRUE)
#########################################################################
# STAT380: Statistical Machine Learning
# Content: Dataset Boston housing
#########################################################################
#Daten<-read.csv("")
Daten<-read.csv("https://raw.githubusercontent.com/subhadippal2019/STAT380UAEU/main/BostonHousing.csv")
#Daten
# Additional information for a quick check to identify whether the data is loaded appropriately
head(Daten, 6) # shows the first 6 rows from the data.
str(Daten) # provides structure of an R object in general. In this case it will show the names of the variables inside it.
summary(Daten) # This commands provides the summary of all the variables of the dataset.
#########################################################################
# 2. Data Partitioning
#########################################################################
# Split data in three partitions (80% Training, 20% Validation)
library(caret)
#install.packages('caret') # Use this command to install the package,  if case the package is not installed and you get an error "rror in library(caret) : there is no package called ‘caret’"
set.seed(10) # An optional argument butr sometimes convinient to set the see to reproduce the result.
#We are going to use the createDataPartition function. Hence see the documentation on the function
# help(createDataPartition)
library(caret)
createDataPartition_alt<-function(y,p = 0.5, list=FALSE){
n=length(y)
Train_size=round(n*p)
sel_sample=sample(x = 1:n,size =Train_size ,replace = FALSE)
return(sel_sample)
}
Daten=na.omit(Daten)
#inTrain = createDataPartition(Daten$CRIM, p = 0.8, list = FALSE)
inTrain = createDataPartition_alt(Daten$CRIM, p = 0.8, list = FALSE)
train = Daten[inTrain, ]
dim(train)
dim(Daten)
test = Daten[-inTrain,]
dim(test)
plot(train$LSTAT, train$MEDV, col="orange", pch=19 , ylab="MEDV", xlab="")
points(test$LSTAT, test$MEDV,col="blue", pch=19)
legend(280, 47, legend=c("train$MEDV",  "test$MEDV"),
col=c("orange" ,"blue"), lty=1:2, cex=0.8)
model_polynomial <- lm(MEDV ~ poly(LSTAT, 2, raw = TRUE), data= train)
#model_linear = lm(MEDV ~ LSTAT, data = train);
summary(model_polynomial)
#Make predictions
library(forecast)
predictions_test =  predict(object =model_polynomial,newdata= test )
#MOdel performance on the Testing Data
data.frame(RMSE = RMSE(predictions_test, test$MEDV), R2 = R2(predictions_test, test$MEDV))
accuracy(predictions_test, test$MEDV)
#MOdel performance on the Training Data
predictions_train =  predict(object =model_polynomial,newdata= train )
data.frame(RMSE = RMSE(predictions_train, train$MEDV), R2 = R2(predictions_train, train$MEDV))
plt<-ggplot(train, aes(LSTAT, MEDV) ) + geom_point() +stat_smooth(method = lm, formula = y ~ poly(x, 2, raw =TRUE))
plt
plot(model_polynomial)
#install.packages("splines")
library(splines)
# Build the model
knots <- quantile(train$LSTAT, p = c(0.25, .5,  0.75))
model_ss1 <- lm (MEDV ~ bs(LSTAT, knots = knots, degree=1),  data =train)
summary(model_ss1)
ggplot(train, aes(LSTAT, MEDV) ) + geom_point() +
stat_smooth(method = lm, formula = y ~ splines::bs(x, knots = knots, degree=1))
#install.packages("splines")
# Build the model
knots <- quantile(train$LSTAT, p = c(0.25, .5,  0.9))
model_ss2 <- lm (MEDV ~ bs(LSTAT, knots = knots, degree=2),  data =train)
summary(model_ss2)
ggplot(train, aes(LSTAT, MEDV) ) + geom_point() +
stat_smooth(method = lm, formula = y ~ splines::bs(x, knots = knots, degree=2))
knitr::opts_chunk$set(echo = TRUE)
#########################################################################
# STAT380: Statistical Machine Learning
# Content: Dataset Boston housing
#########################################################################
#Daten<-read.csv("")
Daten<-read.csv("https://raw.githubusercontent.com/subhadippal2019/STAT380UAEU/main/BostonHousing.csv")
#Daten
# Additional information for a quick check to identify whether the data is loaded appropriately
head(Daten, 6) # shows the first 6 rows from the data.
str(Daten) # provides structure of an R object in general. In this case it will show the names of the variables inside it.
summary(Daten) # This commands provides the summary of all the variables of the dataset.
library(caret)
createDataPartition_alt<-function(y,p = 0.5, list=FALSE){
n=length(y)
Train_size=round(n*p)
sel_sample=sample(x = 1:n,size =Train_size ,replace = FALSE)
return(sel_sample)
}
Daten=na.omit(Daten)
#inTrain = createDataPartition(Daten$CRIM, p = 0.8, list = FALSE)
inTrain = createDataPartition_alt(Daten$CRIM, p = 0.8, list = FALSE)
train = Daten[inTrain, ]
dim(train)
dim(Daten)
test = Daten[-inTrain,]
dim(test)
plot(train$LSTAT, train$MEDV, col="orange", pch=19 , ylab="MEDV", xlab="")
points(test$LSTAT, test$MEDV,col="blue", pch=19)
legend(280, 47, legend=c("train$MEDV",  "test$MEDV"),
col=c("orange" ,"blue"), lty=1:2, cex=0.8)
model_polynomial <- lm(MEDV ~ poly(LSTAT, 2, raw = TRUE), data= train)
#model_linear = lm(MEDV ~ LSTAT, data = train);
summary(model_polynomial)
#Make predictions
library(forecast)
predictions_test =  predict(object =model_polynomial,newdata= test )
#MOdel performance on the Testing Data
data.frame(RMSE = RMSE(predictions_test, test$MEDV), R2 = R2(predictions_test, test$MEDV))
accuracy(predictions_test, test$MEDV)
#MOdel performance on the Training Data
predictions_train =  predict(object =model_polynomial,newdata= train )
data.frame(RMSE = RMSE(predictions_train, train$MEDV), R2 = R2(predictions_train, train$MEDV))
accuracy(predictions_test, test$MEDV)
help("accuracy")
#Make predictions
library(forecast)
accuracy_alt<-function(y, pred_y){
ee<-y-pred_y
Error_Rate<-data.frame(
ME=mean(ee),
RMSE = RMSE(y, pred_y),
MAE=mean(abs(ee)),
MPE=mean((ee/y)),
MAPE=mean(abs(ee/y))
)
return(Error_Rate)
}
predictions_test =  predict(object =model_polynomial,newdata= test )
#MOdel performance on the Testing Data
data.frame(RMSE = RMSE(predictions_test, test$MEDV), R2 = R2(predictions_test, test$MEDV))
accuracy(predictions_test, test$MEDV)
#MOdel performance on the Training Data
predictions_train =  predict(object =model_polynomial,newdata= train )
data.frame(RMSE = RMSE(predictions_train, train$MEDV), R2 = R2(predictions_train, train$MEDV))
accuracy(predictions_test, test$MEDV)
accuracy_alt(predictions_test, test$MEDV)
#Make predictions
library(forecast)
accuracy_alt<-function( pred_y,y){
ee<-y-pred_y
Error_Rate<-data.frame(
ME=mean(ee),
RMSE = RMSE(y, pred_y),
MAE=mean(abs(ee)),
MPE=mean((ee/y)),
MAPE=mean(abs(ee/y))
)
return(Error_Rate)
}
predictions_test =  predict(object =model_polynomial,newdata= test )
#MOdel performance on the Testing Data
data.frame(RMSE = RMSE(predictions_test, test$MEDV), R2 = R2(predictions_test, test$MEDV))
accuracy(predictions_test, test$MEDV)
#MOdel performance on the Training Data
predictions_train =  predict(object =model_polynomial,newdata= train )
data.frame(RMSE = RMSE(predictions_train, train$MEDV), R2 = R2(predictions_train, train$MEDV))
predictions_train =  predict(object =model_polynomial,newdata= train )
accuracy(predictions_test, test$MEDV)
accuracy_alt(predictions_test, test$MEDV)
y
#Make predictions
library(forecast)
accuracy_alt<-function( pred_y,y){
ee<-y-pred_y
Error_Rate<-data.frame(
ME=mean(ee),
RMSE = RMSE(y, pred_y),
MAE=mean(abs(ee)),
MPE=100*mean((ee/y)),
MAPE=100*mean(abs(ee/y))
)
return(Error_Rate)
}
predictions_test =  predict(object =model_polynomial,newdata= test )
#MOdel performance on the Testing Data
data.frame(RMSE = RMSE(predictions_test, test$MEDV), R2 = R2(predictions_test, test$MEDV))
accuracy(predictions_test, test$MEDV)
#MOdel performance on the Training Data
predictions_train =  predict(object =model_polynomial,newdata= train )
data.frame(RMSE = RMSE(predictions_train, train$MEDV), R2 = R2(predictions_train, train$MEDV))
accuracy(predictions_test, test$MEDV)
accuracy_alt(predictions_test, test$MEDV)
#Make predictions
library(forecast)
# library(forecard does not work: try the following)
accuracy_alt<-function( pred_y,y){
ee<-y-pred_y
Error_Rate<-data.frame(
ME=mean(ee),
RMSE = RMSE(y, pred_y),
MAE=mean(abs(ee)),
MPE=100*mean((ee/y)),
MAPE=100*mean(abs(ee/y))
)
return(Error_Rate)
}
predictions_test =  predict(object =model_polynomial,newdata= test )
#MOdel performance on the Testing Data
data.frame(RMSE = RMSE(predictions_test, test$MEDV), R2 = R2(predictions_test, test$MEDV))
accuracy(predictions_test, test$MEDV)
#MOdel performance on the Training Data
predictions_train =  predict(object =model_polynomial,newdata= train )
data.frame(RMSE = RMSE(predictions_train, train$MEDV), R2 = R2(predictions_train, train$MEDV))
