---
title: "BostonHousingExample_Classification"
author: "STAT380"
date: '2023-10-12'
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Loading the BostonHousing Datya

```{r }
### Example 2: Boston data with binary classifier
BostonH<-read.csv(url("https://raw.githubusercontent.com/subhadippal2019/STAT380UAEU/main/BostonHousing.csv"))
set.seed(12)
attach(BostonH)
library(caret)
inTrain = createDataPartition(BostonH$CRIM, p = 0.8, list = FALSE)
train = BostonH[inTrain, ]
vali = BostonH[-inTrain,]
```




```{r }

# Creating scatter plot for the numerical variables:
pairs(BostonH,
             gap = 0,
             bg = c("red", "green", "blue")[as.factor(CAT..MEDV)],
             pch = 21)


```

```{r }
#Discriminant analysis with one predictor: CRIM
library(MASS)
formulas = as.factor(CAT..MEDV) ~ CRIM
MEDV_lda.1 = lda(formulas, train)
p  = predict(MEDV_lda.1, train)
#ldahist(data = p$x[,1], g = train$CAT..MEDV)
###Discriminant analysis density plots
#p.df = data.frame(LD1 = p$x, class = p$class)
#print(p.df) # Not necessary
#ggplot(p.df) + geom_density(aes(LD1, fill = class), alpha = 0.2)
```


```{r }
#### Note: Clearly, based on the histograms and density plots, the prediction was not good.

#Discriminant analysis with all predictors
formulas = as.factor(CAT..MEDV) ~ CRIM + INDUS + NOX + RM + AGE + DIS + PTRATIO + LSTAT + MEDV
MEDV_lda = lda(formulas, train)
MEDV_lda

p  = predict(MEDV_lda, train)
#ldahist(data = p$x[,1], g = train$CAT..MEDV)
#ggord(MEDV_lda, train$CAT..MEDV, ylim =c(-10,10) )
###Discriminant analysis density plots
p.df = data.frame(LD1 = p$x, class = p$class)
print(p.df)
ggplot(p.df) + geom_density(aes(LD1, fill = class), alpha = 0.2)
```


```{r }

####################################################################
# Classifier Performance
###################################################################

## Run logit model: Medianvalue0 /1 ~ LowerPopul.+ Numberofrooms+Teacher /Pupilratio

fit = glm(CAT..MEDV~LSTAT+RM+PTRATIO,data=train,family = "binomial")
summary(fit)

#Create predictions-training vs.validation set
pred_t = predict(fit,newdata = train,type ="response")
pred_v = predict(fit,newdata = vali,type ="response")

#Evaluate performance-training vs.validation set
#Training-logit model
confusionMatrix(as.factor(ifelse(pred_t > 0.5,1,0)), as.factor(train$CAT..MEDV))
#Or
pred_t_c = ifelse(pred_t > 0.5,1,0); head(pred_t_c); length(pred_t_c)
confusionMatrix(as.factor(train$CAT..MEDV), as.factor(pred_t_c))

#Validation-logit model
confusionMatrix(as.factor(ifelse(pred_v>0.5,1,0)), as.factor(vali$CAT..MEDV))

## Validation

#Naive benchmark:the average
y_fit_naive = median(train$CAT..MEDV)
#Create predictions
pred_v_reg = predict(fit,newdata = vali,type ="response")
pred_v_naiv = rep(y_fit_naive,length(vali$MEDV))

#Evaluate performance-validation set
#Validation-logit model vs naive benchmark
confusionMatrix(as.factor(ifelse(pred_v_reg > 0.5, 1,0)), as.factor(vali$CAT..MEDV))

confusionMatrix(as.factor(ifelse(pred_v_naiv > 0.5, 1,0)), as.factor(vali$CAT..MEDV))

```



```{r }
## We check the overall classification accuracy 

predicted.classes = as.factor(ifelse(pred_v_reg > 0.5, 1, 0))
observed.classes  = as.factor(vali$CAT..MEDV)
#Estimated accuracy-logit model
accuracy = mean (observed.classes == predicted.classes)
accuracy

#Estimated miss-classification rate-logit model
error <-mean (observed.classes != predicted.classes)
error

#Confusion matrix, proportion of cases-logit model
table(observed.classes, predicted.classes)
prop.table(table(observed.classes, predicted.classes))
```


```{r }
## We check the graphical representation of the logit accuracy
#Compute the receiver operating characteristics curve (roc)-logit model using library(pROC)

#library(pROC)
#res.roc = roc(observed.classes, pred_v_reg)
#plot.roc(res.roc, print.auc = TRUE)

#### we repeat the same functions with the Iris data example:

### Confusion matrix and accuracy â€“ training data
data("iris")
str(iris)
head(iris)
set.seed(134)
ind = sample(2, nrow(iris), replace = TRUE, prob = c(0.6, 0.4))
training = iris[ind==1,]
testing = iris[ind==2,]

iris_lda = lda(Species~., training)

p1 = predict(iris_lda, training)$class
tab = table(Predicted = p1, Actual = training$Species)
tab

p2 = predict(iris_lda, testing)$class
tab1 = table(Predicted = p2, Actual = testing$Species)
tab1

n = sum(tab) # number of instances
nc = nrow(tab) # number of classes
diag = diag(tab) # number of correctly classified instances per class 
rowsums = apply(tab, 1, sum) # number of instances per class
colsums = apply(tab, 2, sum) # number of predictions per class
p = rowsums / n # distribution of instances over the actual classes
q = colsums / n # distribution of instances over the predicted classes

n = sum(tab1) # number of instances
nc = nrow(tab1) # number of classes
diag = diag(tab1) # number of correctly classified instances per class 
rowsums = apply(tab1, 1, sum) # number of instances per class
colsums = apply(tab1, 2, sum) # number of predictions per class
p = rowsums / n # distribution of instances over the actual classes
q = colsums / n # distribution of instances over the predicted classes

accuracy = sum(diag) / n 

```
