\documentclass[compress]{beamer}
\mode<presentation>
\setbeamercovered{transparent}
\usetheme{Warsaw}
%\useoutertheme{smoothtree}
%\usepackage{pdfpages}
\usepackage{multirow}
\usepackage[english]{babel}
\usepackage[latin1]{inputenc}
\usepackage{times}
\usepackage[T1]{fontenc}
\usepackage{xmpmulti}
\usepackage{multicol}
\usepackage{colortbl}
%\setbeamersize{text margin left=.25 in,text margin right=.25 in}
\setbeamersize{text margin left=.15 in,text margin right=.15 in}
\usepackage[authoryear]{natbib}
\usepackage{epstopdf}
\definecolor{antiquebrass}{rgb}{0.8, 0.58, 0.46}
\definecolor{babyblueeyes}{rgb}{0.63, 0.79, 0.95}
\definecolor{babyblue}{rgb}{0.54, 0.81, 0.94}
\definecolor{bistre}{rgb}{0.24, 0.17, 0.12}
\definecolor{brightlavender}{rgb}{0.75, 0.58, 0.89}
\definecolor{bulgarianrose}{rgb}{0.28, 0.02, 0.03}
\definecolor{slateblue}{rgb}{0.56, 0.74, 0.56}
\definecolor{cordovan}{rgb}{0.54, 0.25, 0.27}
\definecolor{darkbyzantium}{rgb}{0.36, 0.22, 0.33}
\setbeamercolor{structure}{fg=babyblue!70, bg= black!60}
\usepackage{tikz}
\usetikzlibrary{shadows,calc}
\usetikzlibrary{shadows.blur}
\usetikzlibrary{shapes.symbols}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{colortbl}
\usepackage{multirow}
%%%%%%%%% shaddow image %%%%%
% some parameters for customization
\def\shadowshift{3pt,-3pt}
\def\shadowradius{6pt}
\colorlet{innercolor}{black!60}
\colorlet{outercolor}{gray!05}
% this draws a shadow under a rectangle node
\newcommand\drawshadow[1]{
\begin{pgfonlayer}{shadow}
    \shade[outercolor,inner color=innercolor,outer color=outercolor] ($(#1.south west)+(\shadowshift)+(\shadowradius/2,\shadowradius/2)$) circle (\shadowradius);
    \shade[outercolor,inner color=innercolor,outer color=outercolor] ($(#1.north west)+(\shadowshift)+(\shadowradius/2,-\shadowradius/2)$) circle (\shadowradius);
    \shade[outercolor,inner color=innercolor,outer color=outercolor] ($(#1.south east)+(\shadowshift)+(-\shadowradius/2,\shadowradius/2)$) circle (\shadowradius);
    \shade[outercolor,inner color=innercolor,outer color=outercolor] ($(#1.north east)+(\shadowshift)+(-\shadowradius/2,-\shadowradius/2)$) circle (\shadowradius);
    \shade[top color=innercolor,bottom color=outercolor] ($(#1.south west)+(\shadowshift)+(\shadowradius/2,-\shadowradius/2)$) rectangle ($(#1.south east)+(\shadowshift)+(-\shadowradius/2,\shadowradius/2)$);
    \shade[left color=innercolor,right color=outercolor] ($(#1.south east)+(\shadowshift)+(-\shadowradius/2,\shadowradius/2)$) rectangle ($(#1.north east)+(\shadowshift)+(\shadowradius/2,-\shadowradius/2)$);
    \shade[bottom color=innercolor,top color=outercolor] ($(#1.north west)+(\shadowshift)+(\shadowradius/2,-\shadowradius/2)$) rectangle ($(#1.north east)+(\shadowshift)+(-\shadowradius/2,\shadowradius/2)$);
    \shade[outercolor,right color=innercolor,left color=outercolor] ($(#1.south west)+(\shadowshift)+(-\shadowradius/2,\shadowradius/2)$) rectangle ($(#1.north west)+(\shadowshift)+(\shadowradius/2,-\shadowradius/2)$);
    \shade[outercolor,right color=innercolor,left color=innercolor] ($(#1.north west)+(-\shadowradius/12,\shadowradius/12)$) rectangle ($(#1.south east)+(\shadowradius/12,-\shadowradius/12)$);%Frame
    \filldraw ($(#1.south west)+(\shadowshift)+(\shadowradius/2,\shadowradius/2)$) rectangle ($(#1.north east)+(\shadowshift)-(\shadowradius/2,\shadowradius/2)$);
\end{pgfonlayer}
}
% create a shadow layer, so that we don't need to worry about overdrawing other things
\pgfdeclarelayer{shadow} 
\pgfsetlayers{shadow,main}
% Define image shadow command
\newcommand\shadowimage[2][]{%
\begin{tikzpicture}
\node[anchor=south west,inner sep=0] (image) at (0,0) {\includegraphics[#1]{#2}};
\drawshadow{image}
\end{tikzpicture}}
\usepackage{calligra}

%\usepackage{listings}


\DeclareMathOperator*{\argmax}{Arg\,max}
\DeclareMathOperator*{\argmin}{Arg\,min}
\newcommand{\norm}[1]{\left\Vert #1 \right\Vert }
\newcommand{\bbetaHat}{ \widehat{\bbeta}}
\newcommand{\bbetaLSE}{ \widehat{\bbeta}_{_{\text{LSE}}}}
\newcommand{\bbetaMLE}{ \widehat{\bbeta}_{_{\text{MLE}}}}
\newcommand{\sqBullet}[1]{  {\tiny \tiny \tiny \qBoxCol{#1!60}{ }} }
%***************
%\newtheorem{thm}{Theorem}
\input{definition_include.tex}
\input{BoxDef}
\input{MatrixDef}











\title{  STAT 380:\\ {\color{black} Variable Selection,  Ridge,  and Lasso Regression \\
\tiny{ An Example using .... Data}  }}

\author[UAEU]
{}
\institute[UAEU] % (optional, but mostly needed)
{
  \inst{}%
  {\bf United Arab Emirates University}\\
  \vspace{0.1in}

  
}

\date{}


\newcommand{\Xnew}{ \HLTEQ[orange]{X_{_{\text{i}}}} }
\newcommand{\Ynew}{ \HLTEQ[orange]{Y_{_{\text{i}}}} }

%\date{\today}

\AtBeginSection[]
{
  \begin{frame}{Inhalt}
 % \begin{multicols}{1}
	\frametitle{Outline}
    \tableofcontents[currentsection]
  %  \end{multicols}
  \end{frame}
}

\begin{document}
\maketitle




\TransitionFrame[antiquebrass]{\Large Variable Selection in Regression  }
%


{
 \setbeamercolor{background canvas}{bg=}
%\includepdf[pages={1}, scale=1, trim=1mm 5mm 11mm 5mm]{STAT380-Unit2.pdf}
\includepdf[pages={29}, scale=1]{../polynomial Regression/STAT380-Unit2.pdf}
%\includegraphics[scale=1]{../polynomial Regression/STAT380-Unit1.pdf} 
}


\begin{frame}{Forward Selection}
\qbx[4.55in]{teal!40}{ \sqBullet{teal}  Only intercept is considered in the very first model. Thereafter, one variable is {\bf added } to  the model at a time.  Based on the slected model selection criteria. \\
\qBrd[4.1in]{blue!10}{  
Minimum AIC: Comparing  all the models adding one more variable
}\qBrd[4.1in]{olive!10}{  
Minimum BIC: Comparing  all the models adding one more variable
} \qBrd[4.1in]{orange!10}{  
Include the variable in the model taht has Minimum p- value of the corresponding regression coefficient when adding it the existing model.. 
} 
} \\\vspace{.1in}
\qbx[4.55in]{purple!30}{\sqBullet{purple}  The inclusion of the variables are stopped when a pre determined p-value/AIC/BIC is achieved. }\\
\end{frame}


{
 \setbeamercolor{background canvas}{bg=}
%\includepdf[pages={1}, scale=1, trim=1mm 5mm 11mm 5mm]{STAT380-Unit2.pdf}
\includepdf[pages={28}, scale=1]{../polynomial Regression/STAT380-Unit2.pdf}
%\includegraphics[scale=1]{../polynomial Regression/STAT380-Unit1.pdf} 
}




\begin{frame}{Backward Selection}
\qbx[4.55in]{teal!40}{ \sqBullet{teal}  All the variables are considered in the very first model. Thereafter, one variable is removed from the model at a time.  Based on the slected model selection criteria. \\
\qBrd[4.1in]{blue!10}{  
Minimum AIC: Comparing  all the models removing one more variable
}\qBrd[4.1in]{olive!10}{  
Minimum BIC: Comparing  all the models removing one more variable
} \qBrd[4.1in]{orange!10}{  
Each Step remove the variable that has Maximum p- value of the corresponding coefficient.
} 
} \\\vspace{.1in}
\qbx[4.55in]{purple!30}{\sqBullet{purple}  The Elimination of the variables are stopped when a pre determined p-value/AIC/BIC is achieved. }\\
\end{frame}






\TransitionFrame[antiquebrass]{\Large  the data set named `surgical'\\
{\small Available in the R - package library(olsrr)}}







\begin{frame}{ }
%\includegraphics[scale=.4]{PP_Fig1.png} 
\qbx[4.55in]{teal!40}{ \sqBullet{teal}  A dataset containing data about survival of patients undergoing liver operation. 
{\tiny *Kutner, MH, Nachtscheim CJ, Neter J and Li W., 2004}
\vspace{-.12in}
\begin{center}
 \qBrd[2.1in]{blue!10}{ It is a R data-frame with 54 rows and 8 covariates and response is the 'Survival time' } 
 \qBrd[2.1in]{olive!20}{ \HLTY{bcs:} blood clotting score     }\\\vspace{.01in}
\qBrd[2.1in]{orange!20}{ \HLTY{pindex:} prognostic index   }
 \qBrd[2.1in]{olive!20}{ \HLTY{enzyme\_test:} enzyme function test score}\\\vspace{.01in}
\qBrd[4.1in]{orange!20}{ \HLTY{liver\_test:}  liver function test score   }\\
\qBrd[4.1in]{olive!20}{ \HLTY{age:}In years }\\\vspace{.01in}
\qBrd[4.1in]{orange!20}{ \HLTY{Gender:}  lindicator variable for gender (0 = male, 1 = female)  }\\
\qBrd[4.1in]{olive!20}{ \HLTY{alc\_mod:}indicator variable for history of alcohol use (0 = None, 1 = Moderate) }\\\vspace{.01in}
\qBrd[4.1in]{orange!20}{ \HLTY{alc\_heavy:} indicator variable for history of alcohol use (0 = None, 1 = Heavy)  }\\
 \end{center} }
 
 \qbx[4.55in]{blue!40}{ \sqBullet{blue} 
 Therefore, In terms of the regression terminology: $n=120$ and $p=200.$
 }

\end{frame}



\begin{frame}{Forward Selection: R code}

%\qbx[4.55in]{purple!30}{
\includegraphics[scale=.62]{figs8/RScreen Shot1.png} \\
%model <- lm(y ~., data = surgical)
%step_forward<-ols_step_forward_p(model, details = TRUE)
%step_forward
%plot(step_forward)
%}

\end{frame}



\begin{frame}

%\qbx[4.55in]{purple!30}{
\includegraphics[scale=.42]{figs8/Screen Shot5.png} \\
%model <- lm(y ~., data = surgical)
%step_forward<-ols_step_forward_p(model, details = TRUE)
%step_forward
%plot(step_forward)
%}


\end{frame}




\begin{frame}{Backward Selection: R code}


\vspace{.1in}
{
\includegraphics[scale=.6]{figs8/RScreen Shot2.png} }
\end{frame}



\begin{frame}

%\qbx[4.55in]{purple!30}{
\includegraphics[scale=.42]{figs8/Screen Shot4.png} \\
%model <- lm(y ~., data = surgical)
%step_forward<-ols_step_forward_p(model, details = TRUE)
%step_forward
%plot(step_forward)
%}


\end{frame}






\begin{frame}{Selection the best combination comparing all the sebsets}
\qbx[4.55in]{teal!40}{ \sqBullet{teal}  All the $2^p$ models are fitted if there are $p$ variables in total.  \\
\qBrd[4.1in]{blue!10}{  
Minimum AIC: Comparing  all the models best model is chosen based on the AIC/BIC/RMSE/$R^2$ criterion.
}
}
\vspace{2in}
\end{frame}





\begin{frame}

%\qbx[4.55in]{purple!30}{
\includegraphics[scale=.62]{figs8/RScreen Shot3.png} \\
%model <- lm(y ~., data = surgical)
%step_forward<-ols_step_forward_p(model, details = TRUE)
%step_forward
%plot(step_forward)
%}


\end{frame}







\TransitionFrame[antiquebrass]{\Large Penalized Regression }
%

\begin{frame}
\newcommand{\bRdg}{\hat{\beta}_{\text{ridge}, \lambda}}
\DefBoxOne{Penalized Regression: Matrix Notation}{\color{black}
Let $(y_i, \bx_i)\in \R\times \R^{p}$ for $i=1, \ldots n $ are observed data.  A Penalized Regression estimator  is via following minimization (with respect to $\bbeta$) problem:
$$ \DBX{\Vert {\bf y}-\bX\bbeta\Vert^2 +\HLTEQ{\HLTEQ[yellow]{\lambda}  \text{Penalty Function }}}.$$
}\\
\vspace{2in}


\end{frame}



{
 \setbeamercolor{background canvas}{bg=}
%\includepdf[pages={1}, scale=1, trim=1mm 5mm 11mm 5mm]{STAT380-Unit2.pdf}
\includepdf[pages={90-95}, scale=1]{../polynomial Regression/STAT380-Unit2.pdf}
%\includegraphics[scale=1]{../polynomial Regression/STAT380-Unit1.pdf} 
}




\TransitionFrame[antiquebrass]{\Large Ridge Regression }
%


{
 \setbeamercolor{background canvas}{bg=}
%\includepdf[pages={1}, scale=1, trim=1mm 5mm 11mm 5mm]{STAT380-Unit2.pdf}
\includepdf[pages={107}, scale=1]{../polynomial Regression/STAT380-Unit2.pdf}
%\includegraphics[scale=1]{../polynomial Regression/STAT380-Unit1.pdf} 
}





\newcommand{\bRdg}{\hat{\beta}_{\text{ridge}, \lambda}}
\begin{frame}




\DefBoxOne{Ridge Regression: Matrix Notation}{\color{black}
Let $(y_i, \bx_i)\in \R\times \R^{p}$ for $i=1, \ldots n $ are observed data. A Ridge Regression estimator $\bRdg$ is via following minimization problem:

$$\bRdg=\text{Argmin}_{\beta}  \DBX{\Vert {\bf y}-\bX\bbeta\Vert^2 +\HLTEQ{\HLTEQ[yellow]{\lambda} \Vert \beta\Vert^2}}.$$, 
$\lambda>0.$
}\\

\vspace{.1in}
\qBrd[4.5in]{teal!40}{  
\begin{center}
\HLTEQ{\bRdg= (\bX^T\bX+\lambda I_p)^{-1}\bX^T{\bf y},}
\end{center}
}

\end{frame}



\begin{frame}\frametitle{The role of $\lambda$}




\DefBoxOne{Ridge Regression: The objective function}{\color{black}
$$ \DBX{\Vert {\bf y}-\bX\bbeta\Vert^2 +\HLTEQ{\HLTEQ[yellow]{\lambda} \Vert \beta\Vert^2}}.$$
$\lambda>0.$
}\\
\vspace{2in}

\end{frame}


\begin{frame}

%\qbx[4.55in]{purple!30}{
\includegraphics[scale=.37]{figs8/Ridge11.png} \\
%model <- lm(y ~., data = surgical)
%step_forward<-ols_step_forward_p(model, details = TRUE)
%step_forward
%plot(step_forward)
%}
\end{frame}



\begin{frame}

\qbx[4.55in]{teal!40}{ \sqBullet{teal} 
Ridge regression does not exclude variables, but reduces effect
estimates to near zero.  It is therefore not suited for finding
parsimonious models.\\
\qBrd[3.5in]{olive!20}{  
W use the \HLTEQ{\text{glmnet}} r package to fit it. 
}
}
\vspace{2in}


\end{frame}



{
 \setbeamercolor{background canvas}{bg=}
%\includepdf[pages={1}, scale=1, trim=1mm 5mm 11mm 5mm]{STAT380-Unit2.pdf}
\includepdf[pages={80}, scale=1]{../polynomial Regression/STAT380-Unit2.pdf}
%\includegraphics[scale=1]{../polynomial Regression/STAT380-Unit1.pdf} 
}

\begin{frame}{Selection of $\lambda$}

\end{frame}



\begin{frame}

%\qbx[4.55in]{purple!30}{
\includegraphics[scale=.4]{figs8/Screen Shot RidgeCode1.png} \\
%model <- lm(y ~., data = surgical)
%step_forward<-ols_step_forward_p(model, details = TRUE)
%step_forward
%plot(step_forward)
%}


\end{frame}

\begin{frame}\frametitle{Shrinkage Effect}

\vspace{.1in}
\qBrd[4.5in]{teal!40}{  
\begin{center}
\HLTEQ{\bRdg= (\bX^T\bX+\lambda I_p)^{-1}\bX^T{\bf y},}
\end{center}
}
\vspace{2in}

\end{frame}




\begin{frame}

%\qbx[4.55in]{purple!30}{
\includegraphics[scale=.65]{figs8/Ridge_coef.png} \\
%model <- lm(y ~., data = surgical)
%step_forward<-ols_step_forward_p(model, details = TRUE)
%step_forward
%plot(step_forward)
%}


\end{frame}






\begin{frame}

%\qbx[4.55in]{purple!30}{
\includegraphics[scale=.65]{figs8/CVfitRidge.png} \\
%model <- lm(y ~., data = surgical)
%step_forward<-ols_step_forward_p(model, details = TRUE)
%step_forward
%plot(step_forward)
%}


\end{frame}



%
%\begin{frame}{ }
%\qbx[4.55in]{teal!40}{ \sqBullet{teal} Let $\{y_i , {\bf X}_i \}_{i=1}^{n}$
%be the observed data.  And there is a statistical/machine learning  model that provides a prediction for the responses $y_i$. \\
%\qBrd[4.3in]{blue!30}{  We denoted the predicted value for the responses to be $\HLTY{\hat{y}_i}$}
%}
%
%%\qBox{
%
%%}
%
%\end{frame}


\begin{frame}

%\qbx[4.55in]{purple!30}{
\includegraphics[scale=.4]{figs8/Screen Shot RidgeResult.png} \\
%model <- lm(y ~., data = surgical)
%step_forward<-ols_step_forward_p(model, details = TRUE)
%step_forward
%plot(step_forward)
%}


\end{frame}




\TransitionFrame[antiquebrass]{\Large LASSO Regression }
%


{
 \setbeamercolor{background canvas}{bg=}
%\includepdf[pages={1}, scale=1, trim=1mm 5mm 11mm 5mm]{STAT380-Unit2.pdf}
\includepdf[pages={101-102}, scale=1]{../polynomial Regression/STAT380-Unit2.pdf}
%\includegraphics[scale=1]{../polynomial Regression/STAT380-Unit1.pdf} 
}





%\newcommand{\bRdg}{\hat{\beta}_{\text{ridge}, \lambda}}
\begin{frame}




\newcommand{\bLso}{\hat{\beta}_{\text{lasso}, \lambda}}


\DefBoxOne{LASSO Regression: Matrix Notation}{\color{black}
Let $(y_i, \bx_i)\in \R\times \R^{p}$ for $i=1, \ldots n $ are observed data.  A LASSO Regression estimator $\bLso$ is via following minimization problem:
$$\bLso=\text{Argmin}_{\bbeta}  {\bf y}-\bX\bbeta\Vert^2+\HLTEQ{\HLTY{\lambda}\sum_{j=1}^{p}\vert \beta_j \vert}$$ ,
Or 
$$\bLso=\text{Argmin}_{\bbeta}  \DBX{\Vert {\bf y}-\bX\bbeta\Vert^2 +\HLTEQ{\HLTEQ[yellow]{\lambda} \Vert \bbeta\Vert_{_{L_1}}   }}.$$
$\lambda>0$
}\\



\end{frame}



\begin{frame}\frametitle{The role of $\lambda$}




\DefBoxOne{Ridge Regression: The objective function}{\color{black}
$$  \DBX{\Vert {\bf y}-\bX\bbeta\Vert^2 +\HLTEQ{\HLTEQ[yellow]{\lambda} \Vert \bbeta\Vert_{_{L_1}}   }}. $$
$\lambda>0$
}\\
\vspace{2in}

\end{frame}


\begin{frame}

%\qbx[4.55in]{purple!30}{
\includegraphics[scale=.37]{figs8/Lasso11.png} \\
%model <- lm(y ~., data = surgical)
%step_forward<-ols_step_forward_p(model, details = TRUE)
%step_forward
%plot(step_forward)
%}
\end{frame}






\begin{frame}

\qbx[4.55in]{teal!40}{ \sqBullet{teal} 
Lasso regression can estimate a regression coefficient to be Exactly zero,. Therefore it has the ability to perform variable selection.  It is often used to find parsimonious models.\\
\qBrd[3.5in]{olive!20}{  
W use the \HLTEQ{\text{glmnet}} r package to fit it.  
}
}
\vspace{2in}


\end{frame}



\begin{frame}{Selection of $\lambda$}

\end{frame}




\begin{frame}

%\qbx[4.55in]{purple!30}{
\includegraphics[scale=.4]{figs8/LASSO_CODE.png} \\
%model <- lm(y ~., data = surgical)
%step_forward<-ols_step_forward_p(model, details = TRUE)
%step_forward
%plot(step_forward)
%}


\end{frame}









\begin{frame}

%\qbx[4.55in]{purple!30}{
\includegraphics[scale=.5]{figs8/Lasso_coef.png} \\
%model <- lm(y ~., data = surgical)
%step_forward<-ols_step_forward_p(model, details = TRUE)
%step_forward
%plot(step_forward)
%}


\end{frame}






\begin{frame}

%\qbx[4.55in]{purple!30}{
\includegraphics[scale=.55]{figs8/CVfitLasso.png} \\
%model <- lm(y ~., data = surgical)
%step_forward<-ols_step_forward_p(model, details = TRUE)
%step_forward
%plot(step_forward)
%}


\end{frame}

\begin{frame}

%\qbx[4.55in]{purple!30}{
\includegraphics[scale=.4]{figs8/Screen Shot Lasso Results.png} \\
%model <- lm(y ~., data = surgical)
%step_forward<-ols_step_forward_p(model, details = TRUE)
%step_forward
%plot(step_forward)
%}


\end{frame}





\TransitionFrame[antiquebrass]{\Large Regression with Elastic Net Penalty  }
%


{
 \setbeamercolor{background canvas}{bg=}
%\includepdf[pages={1}, scale=1, trim=1mm 5mm 11mm 5mm]{STAT380-Unit2.pdf}
\includepdf[pages={114-115}, scale=1]{../polynomial Regression/STAT380-Unit2.pdf}
%\includegraphics[scale=1]{../polynomial Regression/STAT380-Unit1.pdf} 
}



\begin{frame}

\newcommand{\bEN}{\hat{\beta}_{\text{EN}, \lambda}}


\DefBoxOne{ Regression with Elsatic Net Penalty: Matrix Notation}{\color{black}
Let $(y_i, \bx_i)\in \R\times \R^{p}$ for $i=1, \ldots n $ are observed data.  A Elsatic Net Regression estimator $\bEN$ is via following minimization problem:
$$\bEN=\text{Argmin}_{\bbeta}  \DBX{\Vert {\bf y}-\bX\bbeta\Vert^2 +\HLTEQ{\HLTEQ[yellow]{\lambda}   \left[ \HLTEQ[green]{(\alpha)}     \Vert \bbeta\Vert_{_{L_1}} +   \HLTEQ[green]{(1-\alpha)} \Vert\bbeta\Vert^2      \right] }}.$$
$\lambda>0,  \text{ and } 0\leq \alpha\leq 1.$
}\\



\end{frame}



\TransitionFrame[antiquebrass]{\Large Geneset MicroArray Data}



%
%
%{
% \setbeamercolor{background canvas}{bg=}
%%\includepdf[pages={1}, scale=1, trim=1mm 5mm 11mm 5mm]{STAT380-Unit2.pdf}
%\includepdf[pages={57-61}, scale=1]{../polynomial Regression/STAT380-Unit1.pdf}
%%\includegraphics[scale=1]{../polynomial Regression/STAT380-Unit1.pdf} 
%}


%
%
%\TransitionFrame[antiquebrass]{\Large Polynomial Regression }
%
%{
% \setbeamercolor{background canvas}{bg=}
%%\includepdf[pages={1}, scale=1, trim=1mm 5mm 11mm 5mm]{STAT380-Unit2.pdf}
%\includepdf[pages={55-58}, scale=1]{../polynomial Regression/STAT380-Unit2.pdf}
%}


%\TransitionFrame[antiquebrass]{\Large A  Dataset}




\begin{frame}{ }
%\includegraphics[scale=.4]{PP_Fig1.png} 
\qbx[4.55in]{teal!40}{ \sqBullet{teal}  The covariates are the  allele frequencies of 200 Gene sets for 120 subjects. (Scheetz et al.,
(2006)
\begin{center}
 \qBrd[4.1in]{blue!10}{  It 
represents the data of 120 rats with 200 gene probes.} \\\vspace{.1in}
 \qBrd[4.1in]{olive!20}{  Response a 120-dimensional vector of, which
represents the expression level of `TRIM32' gene. }\\\vspace{.1in}
\qBrd[4.1in]{orange!20}{ We want to idensity which of the other genes are significantly responsible for the gene counts of the `TRIM32'.  }\\
 \end{center} }
 
 \qbx[4.55in]{blue!40}{ \sqBullet{blue} 
 Therefore, In terms of the regression terminology: $n=120$ and $p=200.$
 }

\end{frame}





\TransitionFrame[antiquebrass]{\Huge Thank You}


\end{document}
